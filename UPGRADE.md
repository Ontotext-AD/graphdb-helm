# GraphDB Helm Chart Upgrade Guide

## From 10.x to 11

TODO

## From 9.x to 10

**Warning**: Before starting the migration change your master into read only mode. 
The process is irreversible and full backup is HIGHLY advisable. 
At minimum backup the PV of the worker you are planing to use for migration.

The Helm chart is completely new and not backwards-compatible.

1. Make all masters read only, you can use the workbench.

2. Using the workbench disconnect all repositories of the worker which we are going to use to migrate to 10.0.
   If you've used the official GraphDB helm chart you can select any worker. 
   In case of a custom implementation select one that can easily be scaled down.

   **Note**: Only the repositories that are on the worker will be migrated into the new cluster!

3. Get the PV information of the worker, noting down the capacity and the access mode:
   ```bash
   kubectl get pv
   ```

4. Note down the resource limits of the worker node:
   ```bash
   kubectl get pod graphdb-worker-<selected-worker> -o yaml | grep -B 2 memory
   ```

5. Make sure all the important settings saved in the settings.js of the master are present in the worker's. Their only difference
   should be the lack of locations in the worker's settings.
   ```bash
   kubectl cp graphdb-master-1-0:/opt/graphdb/home/work/workbench/settings.js settings_m.js
   kubectl cp graphdb-worker-<selected-worker>:/opt/graphdb/home/work/workbench/settings.js settings_w.js
   diff settings_m.js settings_w.js
   ```
   If anything other than the locations is different between the files assume that the master's file is correct and copy it to the worker:
   ```bash
   kubectl cp settings_m.js graphdb-worker-<selected-worker>:/opt/graphdb/home/work/workbench/settings.js
   ```

6. During a replication of a node GraphDB 10 can take double the storage which 9.x takes, so you might need to increase your PV size! To do this
   we recommend checking the documentation of your cloud service provider but in general the procedure is:
   - Make sure `allowVolumeExpansion: true` is set in your used storageClass.
   - Request a change in volume capacity by editing your PVC's `spec.resources.requests.storage`
   - Verify the change has taken effect with `get pvc <pvc-name> -o yaml` and checking the `status.capacity` field.

7. Scale down the selected worker. In the official GraphDB every worker has it's' own statefulset.
   List all the stateful sets to find the name of the worker you want to scale down:
   ```bash
   kubectl get statefulsets
   ```
   Then change the number of replicas to 0:
   ```bash
   kubectl scale statefulsets <stateful-set-name> --replicas=0
   ```

8. Once the worker is down patch the worker's PV with `"persistentVolumeReclaimPolicy":"Retain"`:
   ```bash
   kubectl patch pv <worker-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
   ```

9. Delete the worker's PVC.
   ```bash
   kubectl delete pvc <worker-pvc-name>
   ```

10. Patch the PV with `"claimRef":null` so it can go from status Released to Available:
    ```bash
    kubectl patch pv <worker-pv-name> -p '{"spec":{"claimRef":null}}'
    ```

11. Patch the PV with `claimRef` matching the PVC that will be generated by the `volumeClaimTemplates`:
    ```bash
    kubectl patch pv <worker-pv-name> -p '{"spec":{"claimRef":{"name":"graphdb-node-data-dynamic-pvc-graphdb-node-0"}}}'
    ```

12. Create a namespace for the GraphDB 10 helm chart, so we can deploy it without having to delete our 9.x cluster:
    ```bash
    kubectl create namespace <new-namespace-name>
    ```

13. Patch/Move the worker's PV to the new namespace:
    ```bash
    kubectl patch pv <worker-pv-name> -p '{"spec":{"claimRef":{"namespace":"<namespace-name>"}}}'
    ```

14. Create a secret with your license in the new namespace:
     ```bash
    graphdb-license --from-file graphdb.license -n <new-namespace-name>
    ```

15. Install the 10.0 Helm chart. Remember to edit:
    - `graphdb.node.resources.limits.memory` and `graphdb.node.resources.requests.memory` to the ones used by the old workers.
    - `graphdb.nodesCount:` The raft protocol recommends an odd amount of nodes. Set to the amount of workers you had in the old cluster.
    - `graphdb.node.persistance.volumeClaimTemplateSpec.accessModes` and `graphdb.node.persistance.volumeClaimTemplateSpec.resources.requests.storage` to the ones used by the old PVs.
    - `graphdb.clusetConfig.clusterCreationTimeout` high enough so the data from the old worker has time to replicate to all the new nodes. This depends on network speed between the nodes and the read/write performance of the storage. If the replication is expected to take more than 5 minutes add an equivalent `--timeout XXm` to the helm install command.
    - `deployment.host` to temporary address where you can test everything is working.

16. Once you confirm everything has migrated and works as expected you can free up the old `deployment.host` and upgrade the new cluster to it.

**Note**: If you decide to revert to 9.x and don't have a backup of the worker's PV, you won't be able to use the old PV as GraphDB 10's repositories and settings aren't backward compatible.
Your best course of action would be to make sure it will provision a new clean PV, scale the replica back from 0, recreate the worker repositories and reconnect them to the old master repositories letting GraphDB replicate the data.
