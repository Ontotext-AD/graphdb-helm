#
# Main configuration file
#
# To override single property use --set
# To override multiple, provide another values-override.yaml with the -f flag
# See https://helm.sh/docs/chart_template_guide/values_files/
#TODO: fix descriptions, also while at it the readme needs quite the rework

global:
  imagePullSecrets: []
  storageClass: "standard"
  imageRegistry: docker.io

# K8S API versions differ on Kubernetes and local Minikube installation.
# Please, refer to: https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/
versions:
  api: apps/v1
  service: v1
  ingress: networking.k8s.io/v1
  deployment: apps/v1
  statefulset: apps/v1
  secret: v1
  configmap: v1
  volume: v1
  job: batch/v1
  daemon: apps/v1
  pvc: v1
  pv: v1

# Top lvl flat for easier maintenance
images:
  kong:
    repository: kong
    tag: "2.1-alpine"
  graphdb:
    repository: ontotext/graphdb
    tag: "10.0-Tomas-branch"
  busybox:
    repository: busybox
    tag: "1.31"

####### DEPLOYMENT CONFIGURATIONS #######
deployment:
  # -- Defines the policy with which components will request their image.
  imagePullPolicy: IfNotPresent
  # Secret used to pull Docker images. Uncomment to use it.
  # Important: Must be created beforehand
  # imagePullSecret: ontotext5

  # -- The storage place where components will read/write their persistent data in case the default
  # persistent volumes are used. They use the node's file system.
  storage: /data
  # -- The hostname and protocol at which the graphdb will be accessible.
  # Needed to configure ingress as well as some components require it to properly render their UIs
  protocol: http
  # Important: This should be a resolvable hostname, not an IP address!
  host: localhost

  # Configures SSL termination on ingress level.
  # See https://kubernetes.github.io/ingress-nginx/examples/tls-termination/
  tls:
    # -- Feature toggle for SSL termination. Disabled by default.
    enabled: false
    # -- Name of a Kubernetes secret object with the key and certificate.
    # If TLS is enabled, it's required to be provided, depending on the deployment.
    secretName:

  # -- Ingress related configurations
  ingress:
    enabled: true
    class: nginx
    # -- Sets extra ingress annotations
    annotations: {}
    # -- Sets the maximum size for all requests to the underlying Nginx
    maxRequestSize: 512M
    # -- Default timeouts in seconds for the underlying Nginx.
    timeout:
      connect: 5
      read: 60
      send: 60

# KONG API gateway configurations.
# This gateway sits behind the ingress and exposes the rest of the components.
# By default Kong is deployed without database, e.g. stateless mode.
kong:
  enabled: true
  # -- Reference to a configuration map with Kong configurations as environment variables.
  # Override if you need to further configure Kong's system.
  # See https://docs.konghq.com/2.0.x/configuration/
  configmap: kong-configmap
  # -- Reference to a configuration map containing declarative Kong configuration for
  # services and routes. This is the DB-less config.
  # See https://docs.konghq.com/1.5.x/db-less-admin-api/#declarative-configuration
  servicesConfigmap: kong-services-configmap
  # -- Overwrite if you want to deploy Kong on a non-standard port, such as instances
  # where you want to have two different installations on the same hardware.
  port:
    nodePort: 31122
  # -- Global timeout configurations for all services. Values are in milliseconds.
  timeout:
    connect: 60000
    read: 60000
    write: 60000
  # -- Memory cache size configuration for Kong in DB-less mode.
  # Tune according to the given resource limits.
  # See https://docs.konghq.com/2.0.x/configuration/#mem_cache_size
  memCacheSize: "64m"
  # -- Amount of Nginx worker processes. This affects how much memory will be consumed.
  # The auto value will determine the workers based on the available CPUs
  workers: auto
  # Default resource limitations.
  resources:
    limits:
      memory: 1024Mi
  # Schedule and assign on specific node. By default, no restrictions are applied.
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  nodeSelector: {}

# GraphDB database configurations
graphdb:
  clusterConfig:
    nodesCount: 3
    clusterSecret: s3cr37
    # Set cluster config
    electionMinTimeout: 1500
    electionRangeTimeout: 2000
    heartbeatInterval: 500
    messageSize: 64
    verificationMs: 1500
    clusterConfigConfigMap: graphdb-cluster-config-configmap

  # -- References to configuration maps containing settings.js, users.js, graphdb.properties, and logback.xml files to overwrite
  # the default GraphDB configuration. For reference see https://graphdb.ontotext.com/documentation/standard/configuring-graphdb.html
  configs:
    # Override default settings configuration
    #settingsConfigMap: graphdb-settings-configmap
    # Override default users configuration
    #usersConfigMap: graphdb-users-configmap
    # Override default properties configuration
    #propertiesConfigMap: graphdb-properties-configmap
    # Override default logback configuration
    #logbackConfigMap: graphdb-logback-configmap

  security:
    # If the security is enabled, it's mandatory to have a provisioning user, so the healthchecks and cluster linking can work properly
    enabled: false
    provisioningUsername: provisioner
    # bcrypt encrypted password. default: iHaveSuperpowers
    provisioningPassword: iHaveSuperpowers

  node:
    # -- Reference to a configuration map containing one or more .ttl files used for repository
    # initialization in the post install hook. For reference see https://graphdb.ontotext.com/documentation/standard/configuring-a-repository.html
    repositoryConfigmap: graphdb-repo-default-configmap
    # -- Reference to a secret containing 'graphdb.license' file to be used ny the nodes.
    # Important: Must be created beforehand
    license: graphdb-license
    # -- Java arguments with which worker instances will be launched. GraphDB configuration properties can also be passed here in the format -Dprop=value
    java_args: "-XX:MaxRAMPercentage=70 -Ddefault.min.distinct.threshold=100m -XX:+UseContainerSupport"
    # Node scheduling options such as nodeSelector, affinity, tolerations, topologySpreadConstraints can be set here for ALL workers.
    # By default, no restrictions are applied. The same options can be specified per instance in the nodes section.
    # See https://kubernetes.io/docs/concepts/scheduling-eviction

    topologySpreadConstraints:
    # -- Specific GraphDB worker instances configurations. Supported properties for per node configuration are: license, java_args, graphdb_properties
#    nodes:
#      - name: worker-1
#        license: graphdb-license
#      - name: worker-2
#        java_args: "-XX:MaxRAMPercentage=70 -Ddefault.min.distinct.threshold=100m -XX:+UseContainerSupport "
    # -- Persistence configurations.
    # By default, Helm will use a PV that reads and writes to the host file system.
    persistence:
      # -- Name reference prefix of a persistent volume to which the claim will try to attach. If changed, the default PVs won't be used.
      # Example result: graphdb-default-worker-1-pv
#      volumeNamePrefix: graphdb-default-worker
#      storageClassName: standard
#      # -- Storage size request for each worker. The persistent volume has to be able to satisfy the size.
#      storage: 10G
      # use dynamic volume provisioning
      volumeClaimTemplateSpec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
    # -- Below are minimum requirements for data sets of up to 50 million RDF triples
    # For resizing, refer according to your GraphDB version documentation
    # For EE see http://graphdb.ontotext.com/documentation/enterprise/requirements.html
    # Note: Same as for the master node
    resources:
      limits:
        memory: 2Gi
      requests:
        memory: 2Gi
  # Reference to a configuration map with GraphDB specific configurations.
  # Injected as environment variables.
  #configmap: graphdb-configmap
  # GraphDB workbench configurations
  workbench:
    # -- This is the sub path at which GraphDB workbench can be opened.
    # Should be configured in the API gateway (or any other proxy in front)
    subpath: /graphdb

  # Attach additional PV which will be used as an import directory
  # https://graphdb.ontotext.com/documentation/standard/loading-data-using-the-workbench.html#importing-server-files
  import_directory_mount:
    enabled: false
    volumeClaimTemplateSpec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: "10Gi"

  # -- Tools for loading, scanning and repairing data in repos
  # TODO: UPDATE THIS WHEN THE REDESIGN OF ImportRDF and Storage are final
  tools:
    resources:
      limits:
        memory: "10G"
      requests:
        memory: "10G"
    # -- Tool to preload data in a chosen repo
    # https://graphdb.ontotext.com/documentation/enterprise/loading-data-using-preload.html
    preload:
      # -- If trigger is set to true, then the preload tool will be run while initializing the deployment
      # Don't forget to add repo config file(should be named config.ttl) and RDF data file to the graphdb-preload-data-pv
      # (default pv is: /data/graphdb-worker-preload-data)
      trigger: false
      # -- Options to add to the command
      # possible flags: -f, -p, -r
      # If you use the "-f" option, the tool will override the repository and could lose some data.
      flags: "-f"
      rdfDataFile: "geonames_europe.ttl"
    # -- Tool to preload data in a chosen repo
    # https://graphdb.ontotext.com/documentation/enterprise/loading-data-using-the-loadrdf-tool.html
    loadrdf:
      # -- If trigger is set to true, then the loadrdf tool will be run while initializing the deployment
      # Don't forget to add repo config file(should be named config.ttl) and RDF data file to the graphdb-preload-data-pv
      # (default pv is: /data/graphdb-worker-preload-data)
      trigger: false
      # -- Options to add to the command
      # possible flags: -f, -p
      # If you use the "-f" option, the tool will override the repository and could lose some data.
      flags: "-f"
      rdfDataFile: "geonames_europe.ttl"
    persistence:
      #storageClassName: standard
      # -- Storage size request for the preload/loadrdf pv. The persistent volume has to be able to satisfy the size.
      #storage: 10G
      volumeClaimTemplateSpec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"

    # -- Tool for scanning and repairing data
    # See https://graphdb.ontotext.com/documentation/enterprise/storage-tool.html
    storage_tool:
      # -- If trigger is set to true, then the storage tool will be run while initializing the deployment
      trigger: false
      # -- commands to run the storage-tool with
      command: "scan"
      # -- repo to run command on
      repository: "repo-test-1"
      # -- additional options to run the storage-tool with
      options: ""
